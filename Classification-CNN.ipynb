{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "39c0e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"Base\" Kernel for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92395b43",
   "metadata": {},
   "source": [
    "#### Load a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6e850d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labelling</th>\n",
       "      <th>Detokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>olahraga pilates populer deret manfaat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>janice tjen lolos nomor final chennai open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>pasu rsf duga bantai ribu warga sipil sudan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>pertamina peduli salur bantu korban bencana su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>cuaca panas hujan banjir rob bayang wilayah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Labelling                                        Detokenized\n",
       "0        1.0             olahraga pilates populer deret manfaat\n",
       "1        1.0         janice tjen lolos nomor final chennai open\n",
       "2       -1.0        pasu rsf duga bantai ribu warga sipil sudan\n",
       "3        1.0  pertamina peduli salur bantu korban bencana su...\n",
       "4       -1.0        cuaca panas hujan banjir rob bayang wilayah"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform imports and load the dataset:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('(A) Data/PreProcessed_News Content Title_800 Data.csv', usecols=['Detokenized', 'Labelling'], engine='python')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c8aa5c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labelling      2\n",
       "Detokenized    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5fd9dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f9627f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labelling      0\n",
       "Detokenized    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a10bf6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(832, 2)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d4066",
   "metadata": {},
   "source": [
    "#### Take a Quick Look at the Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "26ac8fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labelling\n",
       " 0.0    557\n",
       "-1.0    180\n",
       " 1.0     95\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Labelling'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4c23a",
   "metadata": {},
   "source": [
    "#### Split the Data into Training and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "256ce947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['Detokenized']  \n",
    "\n",
    "# Map labels to 0, 1, 2\n",
    "label_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "y = df['Labelling'].map(label_mapping)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cf535574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling\n",
      "1    557\n",
      "0    180\n",
      "2     95\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f6b50",
   "metadata": {},
   "source": [
    "#### Scikit-learn's CountVectorizer\n",
    "Text preprocessing, tokenizing and the ability to filter out stopwords are all included in [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), which builds a dictionary of features and transforms documents to feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "82d1af1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 3783)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Only the X_train not the whole X\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4e19df",
   "metadata": {},
   "source": [
    "#### Transform Counts to Frequencies with Tf-idf\n",
    "While counting words is helpful, longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid this we can simply divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called **tf** for Term Frequencies.\n",
    "\n",
    "Another refinement on top of **tf** is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called **tf–idf** for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "Both tf and tf–idf can be computed as follows using [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html):\n",
    "\n",
    "==  \n",
    "Because the term \"the\" is so common, term frequency will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms \"red\" and \"dogs\". \n",
    "    \n",
    "An inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.  \n",
    "== ( See \"NLP With Python Notebook\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "81afb7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 3783)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894f1a9",
   "metadata": {},
   "source": [
    "Note: the `fit_transform()` method actually performs two operations: it fits an estimator to the data and then transforms our count-matrix to a tf-idf representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dfc48693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<665x3783 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13940 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03225794",
   "metadata": {},
   "source": [
    "#### Combine Steps with TfidVectorizer\n",
    "In the future, we can combine the CountVectorizer and TfidTransformer steps into one using [TfidVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "afc9debe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 3783)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "37454b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 3783)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf = vectorizer.transform(X_test) # remember to use the original X_train set\n",
    "X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3ca62",
   "metadata": {},
   "source": [
    "#### CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3604213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c9639ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sparse matrix to a dense numpy array\n",
    "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
    "X_test_tfidf_dense = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6d4a8845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 2ms/step - loss: 1.0512\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.9218\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7705\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6502\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5380\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4320\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.3446\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.2741\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.2216\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.1820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21ee57b03d0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(3, activation = 'softmax')    # < softmax activation here\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_tfidf_dense,\n",
    "    y_train,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7df0e7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 25)                94600     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 15)                390       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 3)                 48        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 95,038\n",
      "Trainable params: 95,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6d51f315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_tfidf_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1665c238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 2 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 0 1 0 0\n",
      " 1 1 1 1 1 2 1 1 1 1 0 1 1 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# 2. Convert probabilities to class labels (0, 1, or 2)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab3edb",
   "metadata": {},
   "source": [
    "#### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "86cc25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a9163526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 16,  19,   0],\n",
       "       [  8, 106,   0],\n",
       "       [  2,  14,   2]], dtype=int64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_classes = predictions.argmax(axis=1)\n",
    "confusion_matrix(y_test, y_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8c270f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.46      0.52        35\n",
      "           1       0.76      0.93      0.84       114\n",
      "           2       1.00      0.11      0.20        18\n",
      "\n",
      "    accuracy                           0.74       167\n",
      "   macro avg       0.79      0.50      0.52       167\n",
      "weighted avg       0.76      0.74      0.70       167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
